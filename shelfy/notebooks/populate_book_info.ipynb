{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "import json\n",
    "import sys\n",
    "\n",
    "\n",
    "import numpy\n",
    "\n",
    "\n",
    "from sqlalchemy import create_engine\n",
    "from sqlalchemy_utils import database_exists, create_database\n",
    "import psycopg2\n",
    "\n",
    "\n",
    "import shelfy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Create SQL DB connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a database name (we're using a dataset on births, so we'll call it birth_db)\n",
    "# Set your postgres username/password, and connection specifics\n",
    "username = 'postgres'\n",
    "password = 'password'     # change this\n",
    "host     = 'localhost'\n",
    "port     = '5432'            # default port that postgres listens on\n",
    "db_name  = 'book_info'\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## 'engine' is a connection to a database\n",
    "## Here, we're using postgres, but sqlalchemy can connect to other things too.\n",
    "engine = create_engine( 'postgresql://{}:{}@{}:{}/{}'.format(username, password, host, port, db_name) )\n",
    "print(engine.url)\n",
    "\n",
    "\n",
    "## create a database (if it doesn't exist)\n",
    "if not database_exists(engine.url):\n",
    "    create_database(engine.url)\n",
    "print(database_exists(engine.url))\n",
    "\n",
    "\n",
    "\n",
    "# Create connection and cursor object to insert info into db\n",
    "con = psycopg2.connect(database = db_name, user = username, password = password, host = host)\n",
    "cursor = con.cursor()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Create titles table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create the tables (if don't exist)\n",
    "cursor.execute('''CREATE TABLE IF NOT EXISTS works (\n",
    "                index BIGSERIAL PRIMARY KEY,\n",
    "                titles TEXT);''')\n",
    "\n",
    "cursor.execute('''CREATE TABLE IF NOT EXISTS editions (\n",
    "                index BIGSERIAL PRIMARY KEY,\n",
    "                titles TEXT);''')\n",
    "\n",
    "cursor.execute('''CREATE TABLE IF NOT EXISTS authors (\n",
    "                index BIGSERIAL PRIMARY KEY,\n",
    "                authors TEXT);''')\n",
    "\n",
    "cursor.execute('''CREATE TABLE IF NOT EXISTS publishers (\n",
    "                index BIGSERIAL PRIMARY KEY,\n",
    "                publishers TEXT);''')\n",
    "\n",
    "\n",
    "# Have to commit the table creation\n",
    "con.commit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Fill titles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Works\n",
    "command = '''INSERT INTO works (titles) VALUES (%s);'''\n",
    "\n",
    "\n",
    "titles_path = shelfy.SHELFY_BASE_PATH + '/raw_data/dumps/' + 'ol_dump_works_2017-12-31.txt'\n",
    "\n",
    "\n",
    "num_fails = 0\n",
    "with open(titles_path, 'r') as file_handle:\n",
    "    \n",
    "    for row in file_handle:\n",
    "        try:\n",
    "            title = json.loads(row.split('\\t')[-1])['title']\n",
    "            cursor.execute(command, (title,))\n",
    "            con.commit()\n",
    "        except:\n",
    "            num_fails += 1\n",
    "            print('failed', num_fails)\n",
    "            pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Fill editions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Editions\n",
    "\n",
    "command = '''INSERT INTO editions (titles) VALUES (%s);'''\n",
    "\n",
    "\n",
    "titles_path = shelfy.SHELFY_BASE_PATH + '/raw_data/dumps/' + 'ol_dump_editions_2017-12-31.txt'\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "num_fails = 0\n",
    "with open(titles_path, 'r') as file_handle:\n",
    "    \n",
    "    \n",
    "    i = 0\n",
    "    # Open reader object to parse file\n",
    "    for row in file_handle:\n",
    "        \n",
    "        i += 1\n",
    "        \n",
    "        try:\n",
    "            title = json.loads(row.split('\\t')[4])['title']\n",
    "            cursor.execute(command, (title,))\n",
    "            \n",
    "        except:\n",
    "            num_fails += 1\n",
    "            print('failed', num_fails)\n",
    "            pass\n",
    "        \n",
    "        if i % 10000 == 0:\n",
    "            print('dumping 100000')\n",
    "            con.commit()\n",
    "            i = 0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "##### Fill authors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Editions\n",
    "\n",
    "command = '''INSERT INTO authors (authors) VALUES (%s);'''\n",
    "\n",
    "\n",
    "titles_path = shelfy.SHELFY_BASE_PATH + '/raw_data/dumps/' + 'ol_dump_authors_2017-12-31.txt'\n",
    "\n",
    "\n",
    "num_fails = 0\n",
    "with open(titles_path, 'r') as file_handle:\n",
    "    \n",
    "    \n",
    "    i = 0\n",
    "    # Open reader object to parse file\n",
    "    for row in file_handle:\n",
    "        \n",
    "        i += 1\n",
    "        \n",
    "        try:\n",
    "            author = json.loads(row.split('\\t')[4])['name']\n",
    "            cursor.execute(command, (author,))\n",
    "            \n",
    "        except:\n",
    "            num_fails += 1\n",
    "            print('failed', num_fails)\n",
    "        \n",
    "        if i % 100000 == 0:\n",
    "            print('dumping 100000')\n",
    "            con.commit()\n",
    "            i = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Fill publishers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Publishers\n",
    "\n",
    "command = '''INSERT INTO publishers (publishers) VALUES (%s);'''\n",
    "\n",
    "\n",
    "titles_path = shelfy.SHELFY_BASE_PATH + '/raw_data/dumps/' + 'ol_dump_editions_2017-12-31.txt'\n",
    "\n",
    "\n",
    "num_fails = 0\n",
    "with open(titles_path, 'r') as file_handle:\n",
    "    \n",
    "    \n",
    "    i = 0\n",
    "    # Open reader object to parse file\n",
    "    for row in file_handle:\n",
    "        \n",
    "        i += 1\n",
    "        \n",
    "        try:\n",
    "            publisher = json.loads(row.split('\\t')[4])['publishers'][0]\n",
    "            \n",
    "            cursor.execute(command, (publisher,))\n",
    "            \n",
    "        except:\n",
    "            num_fails += 1\n",
    "            #print('failed', num_fails)\n",
    "        \n",
    "        if i % 100000 == 0:\n",
    "            \n",
    "            print('dumping 100000')\n",
    "            con.commit()\n",
    "            i = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
